---
title: "P8130_final_project"
author: "Group 4: Yue Lai, Aiming Liu, Adeline Shin, Michael Yan"
date: "12/16/2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(dplyr)
library(bestNormalize)
library(arsenal)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Importing and Tidying Data
```{r message = FALSE}
# tidying data and adding a column with the average salary
law_data = read_csv("./data/Lawsuit.csv") %>% 
  janitor::clean_names() %>% 
  mutate(dept = recode_factor(dept, 
                              "1" = "Biochemistry/Molecular Biology", 
                              "2" = "Physiology",
                              "3" = "Genetics", 
                              "4" = "Pediatrics", 
                              "5" = "Medicine", 
                              "6" = "Surgery"),
         gender = recode_factor(gender, 
                                "1" = "Male", 
                                "0" = "Female"),
         clin = recode_factor(clin, 
                              "1" = "Primarily clinical emphasis", 
                              "0" = "Primarily research emphasis"),
         cert = recode_factor(cert, 
                              "1" = "Board certified", 
                              "0" = "Not certified"),
         rank = recode_factor(rank, 
                              "1" = "Assistant", 
                              "2" = "Associate", 
                              "3" = "Full professor"),
         avg_salary = (sal94 + sal95) / 2)
```

# Data Exploration
## Initial Exploration
```{r}
# data exploration pt. 1
my_labels  =  list(dept = "Dept, n(%)", 
                   clin = "Clin, n(%)", 
                   cert = "Cert, n(%)", 
                   prate = "Prate", 
                   exper = "Exper", 
                   rank = "Rank, n(%)", 
                   sal94 = "Sal94", 
                   sal95 = "Sal95",
                   avg_salary = "Average Salary")

my_controls = tableby.control(
               total = T,
               test = T,
               numeric.stats = c("meansd", "medianq1q3"),
               digits = 2,
               digits.pct = 2)

table1 = tableby(gender ~ dept + clin + cert + prate + exper + rank + sal94 + sal95 + avg_salary, data = law_data, control = my_controls)

summary(table1, labelTranslations = my_labels, 
        title = "EDA", text = T) %>% 
  knitr::kable()
```

## Average Salary Exploration
```{r}
# data exploration pt. 2
# investigate the shape of the distribution for variable ‘avg_salary’ and try different transformation
hist(law_data$avg_salary,
     main = "Untransformed Response Variable",
     xlab = "Average salary")
```

Based on the histogram above, we can see a right-skewness of the average salary distribution. Since it is a negative response variable, we are not able to use a BoxCox transformation. Therefore, we will use the function "yeojohnson" from the bestNormalize package to determine the power (lambda) at which the outcome variable needs to be raised.

```{r}
yeojohnson(law_data$avg_salary)
```

From these results, we know to round up lambda and use the Log transformation of the response variable.

```{r}
hist(log(law_data$avg_salary),
main = "Log-Transformed Response Variable",
xlab = "Log-Transformed average salary") 
```

After doing log-transformation, the histogram shows an approximate normal distribution of the response variable, in this case, the log-transformed average salary.

```{r}
law_data_1= read_csv("./data/Lawsuit.csv") %>% 
  janitor::clean_names() 

law_data_cor = law_data_1 %>% 
  mutate(log_sal = log((sal94 + sal95) / 2)) %>% 
  dplyr::select(-id,-sal94,-sal95)
  
library(corrplot)
L_data = cor(as.matrix(law_data_cor))
corrplot(L_data, method = "color",  
         type = "upper", order = "hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col = "darkblue", tl.srt = 45, #Text label color and rotation
         # hide correlation coefficient on the principal diagonal
         diag = FALSE 
         )
```

# Finding Confounders
```{r}
# fit the model with only one variable:gender
law_data_change = law_data %>% 
  select(gender,dept,clin,cert,exper,rank,avg_salary)

fit_sum = lm(log(avg_salary) ~ gender,data = law_data_change) %>% 
  summary() %>% 
  broom::tidy() %>% 
  mutate(model = "log(avg_salary) ~ gender")

# fit all other predicators with gender
p_value_list = vector("list",length = 5)
for (i in 2:6) {
  fitml = 
    as.formula(paste0("log(avg_salary) ~ gender +",names(law_data_change)[i],collapse = ""))
  p_value_list[[i - 1]] = summary(lm(fitml,data = law_data_change ))
}


p_value = vector("list",length = 5)
for (i in 1:5) {
  p_value[[i]] = broom::tidy(p_value_list[[i]]) %>% 
    mutate(
      model = paste0("log(avg_salary) ~ gender +",names(law_data_change)[i + 1],collapse = "")
    )
}

# make a table to show the result
p_value = bind_rows(fit_sum,p_value)
confounder = p_value %>% 
    dplyr::select(model,everything()) %>% 
    filter(term == "genderFemale") %>% 
    mutate(change_rate = round((-0.3853044 - estimate)/(-0.3853044),3)) %>% 
    dplyr::select(model,term,estimate,change_rate)

knitr::kable(confounder)
```

In order to find confounders for the variable `gender`, the general criterion was used: if adding a variable changed the slope for `gender` by 10% or more, it should be considered a confounder.

After running the code, we found that the change in slope was over 10% for `gender` with the following variables: `dept`,`clin`,``cert`,`prate` and `exper`. Therefore, all five of these variables are confounders.

Also, the change in slope of gender when adding `rank` was very close to 10%, so we decided to consider `rank` a confounder as well.

# Finding Interactions
```{r}
# fit all other predicators with gender
# Cody said we could not use this why to find interaction, we should refer some articles. 
interaction_list = vector("list",length = 5)
for (i in 2:6) {
  fitml_inter = 
    as.formula(paste0("log(avg_salary) ~ gender+dept+clin+cert+exper+rank+gender * ",names(law_data_change)[i],collapse = ""))
  interaction_list[[i - 1]] = summary(lm(fitml_inter,data = law_data_change))
}

#fit predicators with gender
interaction = vector("list",length = 5)
for (i in 2:6) {
  fitml_inter = 
    as.formula(paste0("log(avg_salary) ~ gender+dept+clin+cert+exper+rank+gender * ",names(law_data_change)[i],collapse = ""))
  interaction[[i - 1]] = summary(lm(fitml_inter,data = law_data_change))
}

interaction_p_value = vector("list",length = 5)
for (i in 1:5) {
  interaction_p_value[[i]] = broom::tidy(interaction[[i]]) %>% 
    mutate(
      model1 = paste0("log(avg_salary) ~ gender+dept+clin+cert+exper+rank+gender *",names(law_data_change)[i + 1],collapse = "")
    )
}

# make a table to show the results
interaction_p_value = bind_rows(interaction_p_value)
interaction_table = interaction_p_value %>% 
    dplyr::select(model1,everything()) %>% 
    filter(term != "(Intercept)") %>% 
    dplyr::select(model1,everything())

knitr::kable(interaction_table)
```

After referring to some papers, we found that `rank` and `exper`, may have been effect measure modifiers for the association between gender and salary. Therefore, we decided to use ANOVA to test the interactions.

## ANOVA Testing
```{r}
# ANOVA to determine the significance of interaction part
# interaction rank*gender
reg_in_rank = lm(log(avg_salary)~ exper + clin + cert + dept + gender * rank, data = law_data_change)
anova(reg_in_rank)

# interaction exper*gender
reg_in_exper = lm(log(avg_salary)~ gender * exper + rank + cert + dept + clin, data = law_data_change)
anova(reg_in_exper)
```

# Initial Model after Considering Confounders and Interactions
```{r}
# initial model
reg_n_prate = lm(log(avg_salary)~ gender*rank + gender*exper+ clin + dept + cert , data = law_data_change) 
summary(reg_n_prate)
```

## Model Assumption Validation
Linear models rely on the following assumptions about the residuals:
1. Normally Distributed
2. They have the same variance at every predictor (Homoscedasticity)
3. They are independent of one another

These assumptions were tested using the following plots:

```{r}
# find outliers/influential points
law_model = lm(log(avg_salary)~ gender * rank + dept + clin + cert + exper * gender, data = law_data_change)

par(mfrow = c(2, 2))
plot(law_model)
```

As shown from these diagnostic graphs, point 184 was an outlier, and it was therefore removed from the overall dataset.

## Removing an Outlier
```{r}
# delete the outlier and build a new dataset
law_trans_minus = law_data[-c(184),]
```

After removing this outlier, the final model could be built with the updated dataset. Since there were effect measure modifiers found, the results were stratified by each of the variables found to have interaction earlier (`rank`, `exper`).

# Stratification of Model Results for Effect Measure Modifiers
## Stratification by Rank
```{r}
# seperate regression models for rank
rank_assi_minus = 
  law_trans_minus %>% 
  filter(rank == "Assistant") 

rank_asso_minus =
  law_trans_minus %>% 
  filter(rank == "Associate")

rank_prof_minus = 
  law_trans_minus %>% 
  filter(rank == "Full professor")

# Rank = Assistant: gender not significant
fit_assi_minus = lm(log(avg_salary)~ gender*exper +clin + dept+ cert, data = rank_assi_minus)
summary(fit_assi_minus)

# Rank = Associate: gender significant
fit_asso_minus = lm(log(avg_salary)~ gender*exper + clin + dept+ cert, data = rank_asso_minus)
summary(fit_asso_minus)

# Rank = Full professor: gender not significant
fit_prof_minus = lm(log(avg_salary)~  gender*exper + clin + dept+ cert, data = rank_prof_minus)
summary(fit_prof_minus)
```

As shown from these separate tables, there was a significant difference in salary among genders at the associate professor level, but not the assistant professor or full professor levels. The plot below shows these results graphically.

```{r}
# plot showing the interaction of rank
plot_rank = 
  ggplot(aes(x = gender,y = log(avg_salary),colour = rank, fill =rank, group = rank),data = law_trans_minus)+
    geom_point(position ="jitter",size =2,  alpha =0.4)+
    geom_smooth(se = FALSE, fill ="lightgrey",method ="lm",alpha =0.6,size =1.2)+
    scale_colour_viridis_d("Rank")+
    scale_fill_viridis_d("Rank") +
    labs(title = "Rank",y="salary") +
    theme(plot.title = element_text(hjust = 0, size = 10), axis.text.x = element_text(size = 9), axis.title.x = element_text(size = 10), axis.text.y = element_text(size = 9), axis.title.y = element_text(size = 10), legend.position = "none")

plot_rank
```

## Stratification by Years of Experience
```{r}
# build a new dataset to stratify exper
law_data_exper = law_trans_minus %>% 
  mutate(exper = case_when(exper <= 6 ~ "exper_6",
                           exper > 6 & exper <= 9 ~ "exper_6_9",
                           exper > 9 & exper <= 14 ~ "exper_9_14",
                               exper >= 14 ~ "exper_14"))  
```

```{r}
# seperate regression model for exper (according to Q1, median, Q3)
exper_6_minus = 
  law_data_exper %>% 
  filter(exper == "exper_6") 

exper_6_9_minus =
  law_data_exper %>% 
  filter(exper == "exper_6_9") 

exper_9_14_minus = 
  law_data_exper %>% 
  filter(exper == "exper_9_14") 

exper_14_minus = 
  law_data_exper %>% 
  filter(exper == "exper_14") 

# exper < 6
fit_6_minus = lm(log(avg_salary)~ gender*rank + dept + clin + cert, data = exper_6_minus)
summary(fit_6_minus)

# 6 < exper <= 9
fit_6_9_minus = lm(log(avg_salary)~ gender*rank + dept + clin + cert, data = exper_6_9_minus)
summary(fit_6_9_minus)

# 9 < exper <= 14
fit_9_14_minus = lm(log(avg_salary)~ gender*rank + dept + clin + cert, data = exper_9_14_minus)
summary(fit_9_14_minus)

# exper > 14
fit_14_minus = lm(log(avg_salary)~ gender*rank + dept + clin + cert, data = exper_14_minus)
summary(fit_14_minus)
```

As shown in the summaries of the stratification for years of experience, gender is not a significant variable in any of the stratified models. These results are also shown in the plot below.

```{r}
# plot showing the interaction of exper
plot_exper = 
  ggplot(aes(x = gender,y = log(avg_salary),colour = exper, fill = exper, group = exper),data = law_data_exper)+
    geom_point(position ="jitter",size =2,  alpha =0.4)+
    geom_smooth(se = FALSE, fill ="lightgrey",method ="lm",alpha =0.6,size =1.2)+
    scale_colour_viridis_d("Exper")+
    scale_fill_viridis_d("Exper") +
    labs(title = "Exper",y="salary") +
    theme(plot.title = element_text(hjust = 0, size = 10), axis.text.x = element_text(size = 9), axis.title.x = element_text(size = 10), axis.text.y = element_text(size = 9), axis.title.y = element_text(size = 10), legend.position = "none")

plot_exper
```

# Final Model
After determining which variables were confounders and effect measure modifiers, as well as removing outliers, the final recommended model is:

```{r}
#final model after deleting the outlier
final_law_model = lm(log(avg_salary)~ gender * rank + dept + clin + cert +  gender*exper, data = law_trans_minus)
summary(final_law_model)
```

Below are the plots to test the assumptions for the final model:

```{r}
par(mfrow = c(2, 2))
plot(final_law_model)
```

As seen, the Residuals vs. Fitted graph has a nearly straight line at 0, meaning that the assumption of homoscedascity is valid. This is confirmed by the straight line in the Scale-Location graph as well. The Normal Q-Q Plot shows a straight line that does not have any outliers. Finally, the Residuals vs. Leverage graph shows us that there are no influential cases, as all points are well within Cook's distance, and the dashed line for Cook's distance cannot even be seen in the graph.
